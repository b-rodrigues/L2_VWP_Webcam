---
title: "Language Without Borders: A Step-by-Step Guide to Analyzing Webcam Eye-Tracking Data for L2 Research"
# If blank, the running header is the title in upper case.
shorttitle: "VWP Webcam Tutorial"
# Set names and affiliations.
# It is nice to specify everyone's orcid, if possible.
# There can be only one corresponding author, but declaring one is optional.
author:
  - name: Jason Geller
    corresponding: true
    orcid: 0000-0002-7459-4505
    email: jason.geller@bc.edu
    url: www.drjasongeller.com
    # Roles are optional. 
    # Select from the CRediT: Contributor Roles Taxonomy https://credit.niso.org/
    # conceptualization, data curation, formal Analysis, funding acquisition, investigation, 
    # methodology, project administration, resources, software, supervision, validation, 
    # visualization, writing, editing
    roles:
      - conceptualization
      - writing
      - data curation
      - editing
      - software
      - formal analysis
      - project administration
    affiliations:
      - id: id1
        name: "Boston College"
        department: Department of Psychology and Neuroscience
        address: Mcguinn Hall 405
        city: Chestnut Hill
        region: MA
        country: USA
        postal-code: 02467-9991

  - name: Yanina Prystauka
    orcid: 0000-0000-0000-0002
    roles:
      - writing
      - editing
      - formal analysis
    affiliations: 
      - id: id2
        name: "University of Bergen"
        department: Department of Psychology and Neuroscience
  - name: Sarah Colby
    orcid: 0000-0000-0000-0003
    roles: 
      - writing
      - editing
    affiliations:
     - id: id3
       name: "University of Ottowa"
       department: Department of Psychology and Neuroscience
  - name: Julia Droulin
    orcid: 0000-0000-0000-0003
    roles: 
      - conceptualziation
      - writing
      - editing
      - funding acquisition
    affiliations:
     - id: id4
       name: "University of North Carolina at Chapel Hill"
       department: Department of Psychology and Neuroscienc
author-note:
  status-changes: 
    # Example: [Author name] is now at [affiliation].
    affiliation-change: Carina Mengano is now at Generic University.
    # Example: [Author name] is deceased.
    deceased: ~
  # Disclosures condensed to one paragraph, but you can start a field with two line breaks to break them up: \n\nNew Paragraph
  disclosures:
    # Example: This study was registered at X (Identifier Y).
    study-registration: ~
    # Acknowledge and cite data/materials to be shared.
    data-sharing: ~
    # Example: This article is based on data published in [Reference].
    # Example: This article is based on the dissertation completed by [citation].  
    related-report: ~
    # Example: [Author name] has been a paid consultant for Corporation X, which funded this study.
    conflict-of-interest: The authors have no conflicts of interest to disclose.
    # Example: This study was supported by Grant [Grant Number] from [Funding Source].
    financial-support: ~
    # Example: The authors are grateful to [Person] for [Reason].
    gratitude: ~
    # Example. Because the authors are equal contributors, order of authorship was determined by a fair coin toss.
    authorship-agreements: ~
abstract: "Eye-tracking has become a valuable tool for studying cognitive processes in second language (L2) acquisition and bilingualism (Godfroid et al., 2024). While research-grade infrared eye-trackers are commonly used, their cost can limit accessibility. Recently, webcam-based eye-tracking has emerged as a more affordable alternative, requiring only internet access and a personal webcam. However, webcam-based eye-tracking presents unique design and preprocessing challenges that must be addressed for valid results. To help researchers overcome these challenges,we developed a comprehensive tutorial focused on using webcam eye-tracking for language (L1 and L2) research. Our guide will cover all key steps, from experiment design to data preprocessing and analysis, where we highlight the R package webgazeR, which is open source and freely available for download and installation: . We will offer best practices for environmental conditions, participant instructions, and tips for designing Visual World Paradigm (VWP) experiments with webcam eye-tracking. To demonstrate these steps, we analyze data collected through the Gorilla platform (Anwyl-Irvine et al., 2020) from L1-English (Experiment 1) and L2-Spanish speakers (Experiment 2) in a Visual World Paradigm. This tutorial aims to empower researchers by providing a step-by-step guide to successfully conduct webcam-based eye-tracking studies."
# Put as many keywords at you like, separated by commmas (e.g., [reliability, validity, generalizability])
keywords: [VWP, Tutorial, Webcam Eye-tracking, R]
# If true, tables and figures are mingled with the text instead of listed at the end of the document.
floatsintext: true
# Numbered lines (.pdf and .docx only)
numbered-lines: false
# File with references
bibliography: references.bib
# Suppress title page
suppress-title-page: false
# Link citations to references
link-citations: true
# Masks references that appear in the masked-citations list
mask: false
masked-citations:
# If true, adds today's date below author affiliations. If text, can be any value.
# This is not standard APA format, but it is convenient.
# Works with docx, html, and typst. 
draft-date: false
# Language options. See https://quarto.org/docs/authoring/language.html
lang: en
language:
  citation-last-author-separator: "and"
  citation-masked-author: "Masked Citation"
  citation-masked-date: "n.d."
  citation-masked-title: "Masked Title"
  email: "Email"
  title-block-author-note: "Author Note"
  title-block-correspondence-note: "Correspondence concerning this article should be addressed to"
  title-block-role-introduction: "Author roles were classified using the Contributor Role Taxonomy (CRediT; https://credit.niso.org/) as follows:"
  references-meta-analysis: "References marked with an asterisk indicate studies included in the meta-analysis."
format:
  apaquarto-typst: default
    code-line-numbers: true

execute: 
  echo: true
  warning: false
  message: false
---

# Introduction

Eye-tracking, a technology dating back over a century, has undergone remarkable advancements. Initially, it was an invasive technique requiring cups or contact lenses, often necessitating anesthesia (). However, it has evolved into a non-invasive, lightweight, and user-friendly solution, widely available in laboratories worldwide.

Despite its widespread use in research, several obstacles hinder its adoption. One significant challenge is the specialized expertise required to operate eye-tracking equipment. Remote, high-speed eye trackers are prevalent in many labs, but not everyone can afford to own one. Eye trackers can be expensive, ranging from a few thousand dollars (e.g., gazePoint) to tens of thousands of dollars (e.g., Tobii, SR-Desaexh).

In response to this, individuals have increasingly turned to personal computers and webcams as alternative eye-tracking solutions. Webcameras, either purchased or pre-installed on many computers, can be used to track eye gaze. WebGazer (), a JavaScript library, plays a crucial role in this process. WebGazer.js utilizes a webcam to track eye movements in real time, without relying on pupil or corneal reflection. Instead, it employs facial feature detection to track the user’s head and eye position. By analyzing the relative movement of the eyes, WebGazer.js employs machine learning to estimate the user’s gaze location on the screen.

To achieve accurate tracking, WebGazer.js initiates a quick calibration process, where the user follows a dot on the screen. This calibration process enhances the library’s accuracy. WebGazer.js provides gaze coordinates that can be used to control interfaces or gather data on visual attention. While it may not match the precision of specialized hardware, WebGazer.js offers a simple and accessible solution for browser-based eye tracking.

Other algorithms have also been developed to enhance eye-tracking capabilities, each with its own strengths and applications.

Potential lies primarily in the education and healthcare sectors, where digital transformation has been ongoing for some time, accelerated by the pandemic and the continued need for remote collaboration capabilities.

A significant amount of work has been invested in validating webcam eye tracking. One area that has garnered considerable attention is the application of webcam eye tracking to language research. The results are promising, with many studies showing a strong correlation between findings from webcam eye tracking and lab-based studies using research-grade eye trackers. For example, using Webgazer VYanina, et al.

Eye tracking has provided users with profound insights into the human mind. However, it has left an indelible mark in the domain of language processing.

The purpose of this tutorial is to provide an overview of the basic set-up and design features of an online VWP task and to highlight the preprocessing steps needed to analyze webcam eye-tracking data. Here we use the popular open source programming language R (cite) and introduce the `webgazeR` pacakage to facilitate pre-processing of webcam data. To highlight the package we present data from a cross-lingustic VWP with L2 spanish speakers we collected using Gorilla and Porlific. To helo researchers on their webcam eye-tracking journey all tasks, materials, code, and data are available and open.

# Experiment 1: Monololingual VWP

## Method

All tasks herein can be previewed here (<https://app.gorilla.sc/openmaterials/917915>). The manuscript, data, R code, can be found on Github (<https://github.com/jgeller112/webcam_gazeR_VWP>) and OSF ().

### Participants

Forty participants signed up online using the Gorilla hosting and experiment platform. @tbl-demo lists basic demographic information of the recruited participants. Participants were recruited with Prolific (prolific.co) and paid for their time. Using a pre-screening questionnaires on Prolific, we limited our sample to participants from the USA, aged between 18 and 47, and native English speakers. Participants were also screened to ensure they wore headphones during the task and had no history of self-reported neurological, visual, or auditory issues. Due to calibration failures (16) and headphone failures (1) we ended up with 23 usable participants.

```{r}
#| label: tbl-demo
#| tbl-cap: Demographic variables Experiment 1
#| echo: false
#| after-caption-space: 0pt

library(tidyverse)
library(gtsummary)
library(gt)


quest1 <- read_csv(here::here("data", "monolinguals", "data_exp_189729-v5_questionnaire-rixu.csv"))
quest2 <- read_csv(here::here("data", "monolinguals", "data_exp_189729-v6_questionnaire-rixu.csv"))

quest <- rbind(quest1, quest2)

quest %>%
    select(`Participant Private ID`, `Question Key`, Response, `Participant Browser`) %>%
    drop_na() %>%
    filter(
        !`Question Key` %in% c("END QUESTIONNAIRE", "BEGIN QUESTIONNAIRE") &
            !str_detect(`Question Key`, "quantised")
    ) %>%
    pivot_wider(names_from = `Question Key`, values_from = Response) %>%
    rename(
        Age = `response-1`,
        Gender = `response-2`,
     `Spoken dialect` = `response-3`,
        Race = `response-4`,
     Browser=`Participant Browser`
    ) %>%
  mutate(Age = as.numeric(Age),
      Browser = gsub("[0-9]", "", Browser), 
      Browser= gsub("\\.+$", "", Browser)) %>% 
    select(Age, Gender, `Spoken dialect`, Race, Browser) %>%
  tbl_summary(statistic = list(
      Age ~ "({min}, {max}), {mean}({sd})", 
      all_categorical() ~ "{n} / {N} ({p}%)"
    ),) %>%
  bold_labels()%>%
  as_gt() %>%
    gt::gtsave( # save table as image
    filename = "my_demo.png"
  )


knitr::include_graphics("my_demo.png")

```

### Materials

#### VWP

##### Items

Stimuli were adopted from @colby2023. Stimuli sets were comprised of a target, onset (cohort) competitor, rhyme competitor, and an unrelated item (e.g., *rocket*, *rocker*, *pocket*, *bubble*). The original sets were developed over the course of a series of pilot studies intended to show the most protypical pattern of competition. The major goal was to build a canonical VWP task. For the webcam version of the task we shortened their stimuli sets from 60 (30 monosyllabic sets and 30 bisyllabic sets) to 30 (15 monosyllabic sets and 15 bisyllabic sets).

Each of the four items from a set was used as the auditory target once. In addition, one additional item from each set was randomly selected to serve as the target word on an additional trial to discourage participants from predicting the upcoming target word once the items in the display are visible (i.e., they cannot assume that once they have already heard *rocket, rocker* must be the target). This led to a total of 150 trials (30 sets × 5 targets/set) . Picture location was pseudo-randomized across trials and participants, such that each image type was equally likely to appear in any quadrant. A Matlab script to do this can be found here:. For this experiment we examined four different trial conditions: Target-Cohort-Rhyme-Unrelated, Target-Cohort-Unrelated-Unrelated, Target-Rhyme-Unrelated-Unrelated, and Target-Unrelated-Unrelated-Unrelated.

##### Stimuli

Auditory stimuli were recorded by a female monolingual speaker of English in a sound-attenuated room sampled at 44.1 kHz. Auditory tokens were edited to reduce noise and remove clicks. They were then amplitude normalized to 70 dB SPL. Visual stimuli were images from a commercial clipart database that were selected by a consensus method involving a small group of students. All .wav files were converted to .mp3 for online data collection.

### Headphone screener

The headphones screener is a six-trial task taken from On each trial, three tones of the same frequency and duration were presented sequentially. One tone had a lower amplitude than the other two tones. Tones were presented in stereo, but the tones in the left and right channels were 180 out of phase across stereo channels—in free field, these sounds should cancel out or create distortion, whereas they will be perfectly clear over headphones. The listener picked which of the three tones was the quietest. Performance is generally at the ceiling when wearing headphones but poor when listening in the free field (due to phase cancellation).

#### Demographics questionnaire

Participants completed a demographic questionnaire as part of the study. The questions covered basic demographic information, including age, gender, spoken dialect, ethnicity, and race.

Participants also answered a series of questions related to their personal health and environmental conditions during the experiment. These questions addressed any history of vision problems (e.g., corrected vision, eye disease, or drooping eyelids) and whether they were currently taking medications that might impair judgment. Participants also indicated if they were wearing eyeglasses, contacts, makeup, false eyelashes, or hats.

The questionnaire inquired about their environment, asking if there was natural light in the room, if they were using a built-in camera or an external one (with an option to specify the brand), and their estimated distance from the camera. Participants were asked to estimate how many times they looked at their phone or got up during the experiment and whether their environment was distraction-free.

Additional questions assessed the clarity of calibration instructions, allowing participants to suggest improvements, and asked if they were wearing a mask during the session. These questions aimed to gather insights into personal and environmental factors that could impact data quality and participant comfort during the experiment.

Procedure

All tasks were completed in a single session, lasting approximately 30 minutes. The tasks were presented in a fixed order: consent, headphone screener, spoken word Visual World Paradigm (VWP), and a questionnaire.

The experiment was programmed in the Gorilla Experiment Platform (Anwyl-Irvine et al., 2019), with personal computers as the only permitted device type. Upon entering the online study, participants received general information to decide if they wished to participate, after which they provided informed consent. Participants were then instructed to adjust the volume to a comfortable level while noise played.

Next, participants completed a headphone screening test. They had three attempts to pass this test. If unsuccessful by the third attempt, they were excluded from the experiment.

For those who passed the screening, the next task was the VWP. This began with instructional videos providing specific guidance on the ideal experiment setup for eye-tracking and calibration procedures. Participants were then required to enter full-screen mode before calibration. Calibration occurred every 50 trials for a total of 2 calibrations. Participants had three attempts to successfully complete each calibration phase. If calibration was unsuccessful, participants were directed to an early exit screen, followed by the questionnaire.

In the main VWP task, following video instructions, participants completed four practice trials to familiarize themselves with the procedure. Each trial began with a 500 ms fixation cross at the center of the screen. This was followed by a preview screen displaying four images, each positioned in a corner of the screen. After 1500 ms, a start button appeared in the center. Participants clicked the button to confirm they were focused on the center before the audio played. Once clicked, the audio was played, and the images remained visible. Participants were instructed to click the image that best matched the spoken target word, while their eye movements were recorded throughout the trial.

After completing the main VWP task, participants proceeded to the final questionnaire, which included questions about the eye-tracking task and basic demographic information. Participants were then thanked for their participation.

### Preprocessing Data

The main focus of this tutorial is how to preprocess webcam eye-tracking data. Below we will highlight the steps needed to analyze webcam eye-tracking. For some of this preprocessing we will use the newly created `webgazeR`package (v. 0.1.0) which is an extension of the `gazeR` package (Geller et al., 2020) which was created to analyze VWP data in lab-based studies.

For preprocessing webcam eye data, we will follow five general steps:

1.  Reading in data
2.  Combining trial- and eye-level data
3.  Assigning areas of interest
4.  Time Binning
5.  Aggregating (optional)

For each of these steps, we will display R code chunks demonstrating how to perform each step with helper functions (if applicable) from the webgazeR package in R.

## Reading in Data

### Package Installation and Setup

Before turning to the pre-processing code below, we will need to make sure all the necessary packages are installed. The code will not run if the packages are not installed properly. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).

#### webgazeR installation

The `webgazeR` package can be installed along with helper pack- ages using the remotes package:

```{r}
#| label: webgazer

remotes::install_github("jgeller112/webgazeR")

```

Once this is installed, webgazeR can be loaded along with additional useful packages:

```{r}
#| load: packages
#|
options(stringsAsFactors = F)          # no automatic data transformation
options("scipen" = 100, "digits" = 10) # suppress math annotation
library(tidyverse)
library(remotes) # install github repo
library(here) # relative paths instead of abosoulte aids in reproduce
library(tinytable) # nice tables
library(janitor)# functions for cleaning up your column names
library(webgazeR) # has webcam functions
library(readxl)
library(ggokabeito)
library(optimx)

```

Once webgazeR and other helper packages have been installed and loaded the user is ready to start preprocessing data.

### Gorilla Data

### Behavioral, trial-level, data

To process eye-tracking data you will need to make sure you have both the behavioral data and the eye-tracking data files. For the behavioral data, Gorilla produces a `.csv` file that include trial-level information (here contained in the object `agg_ege_data)`.

Gorilla produces a `.csv` file that include trial-level information (here contained in the object `agg_ege_data)`. For our experiment the behavioral data is stored in the data folder. You will find the files you need in the monolingual subfolder. They are called `data_exp_189729-v5_task-yxtu.csv.` and `data_exp_189729-v6_task-yxtu.csv.`

The .csv files contain meta-data for each each trial, such as what picture were presented on each trial, which object was the target, reaction times, audio presentation times, what object was clicked on, etc. To load our data files into our R environment, we use the `here` package to set a relative rather than an absolute path to our files. The below object `agg_eye_data` merges both `data_exp_189729-v5_task-yxtu.csv` and `data_exp_189729-v6_task-yxtu.csv` into one object [^1]

[^1]: We ran a slightly modified version of the experiment due to an error in branching which is why we have two files

```{r}
#| label: trial data
# load in trial level data
agg_eye_data <- read.csv(here::here("data","monolinguals", "data_exp_189729-v5_task-yxtu.csv"))

agg_eye_data_1<-  read.csv(here::here("data","monolinguals", "data_exp_189729-v6_task-yxtu.csv"))

agg_eye_data <- rbind(agg_eye_data, agg_eye_data_1) # bind the two datasets together

```

### Eye-tracking data

Gorilla currently saves each participant's eye-tracking data in separate files for each trial. The `raw` folder in the project repository contains files by particpant for each trial individually. In these files, we have information pertaining to each trial such as `participant id`,`time since trial started`, `x`and `y` coordinates of gaze, convergence (the model’s confidence in finding a face (and accurately predicting eye movements), face confidence (represents the support vector machine (SVM) classifier score for the face model fit), and information pertaining to the the AOI screen coordinates (standardized and user-specific). The `vwp_files` object below contains all our files. Because`vwp_files` contains trial data as well as calibration data, we remove the calibration trails and save the files to `vwp_paths_filtered`.

```{r}
#| label: eye data
# Get the list of all files in the folder
vwp_files  <- list.files(here::here("data", "monolinguals", "raw"), pattern = "\\.xlsx$", full.names = TRUE)

# Exclude files that contain "calibration" in their filename
vwp_paths_filtered <- vwp_files[!grepl("calibration", vwp_files)]

```

When data is generated from Gorilla, each trial in your experiment is saved as an individual file. Because of this, we need some way to take all the individual files and merge them together. The `merge_webcam_files()`function merges each trial from each participant into a single tibble or dataframe. Before running the `merge_webcam_files()` function, ensure that your working directory is set to where the files are stored. `merge_webcam_files()` reads in all the .xlsx files, binds them together into one dataframe, and cleans up the column names. The function then filters the data to include only rows where the type is "prediction" and the `screen_index` matches the specified value (in our case, screen 4 is where we collected eye-tracking data). If you recorded across multiple screens the `screen_index` argument can take multiple values (e.g., `screen_index`= c(1, 4, 5)). `merge_webcam_files()` also renames the `spreadsheet_row` column to trial and sets both `trial` and `subject` as factors for further analysis in our pipeline. As a note, all steps should be followed in order due to the renaming of column names. If you encounter an error it might be because column names have not been changed.

```{r}
setwd(here::here("data", "monolinguals", "raw")) # set working directory to raw data folder

edat <- merge_webcam_files(vwp_paths_filtered, screen_index=4) # eye tracking occured ons creen index 4

```

## Subject and trial level data removal

To ensure high-quality data, it is essential to filter out unreliable data based on both behavioral and eye-tracking criteria before merging datasets. Participants will be excluded if they meet any of the following conditions: failure to successfully calibrate throughout the experiment, low accuracy, low sampling rates, and a high proportion of gaze data outside the screen coordinates. Successful calibration is crucial for capturing accurate eye-tracking measurements, so participants who could not maintain proper calibration may have inaccurate gaze data. Similarly, low accuracy may indicate poor engagement or task difficulty, which can reduce the reliability of the behavioral data and suggest that eye-tracking data may be less precise.

First, we will create a cleaned up version of our use the behavioral, trial-level, data `agg_eye_data` by creating an object named `eye_behav` that selects useful columns from that file and renames stimuli to make them more intuitive. Because most of this will be user-specific, no function is called here. Below we describe the pre-processing done on the behavioral data file. The below code processes and transforms the `agg_eye_data` dataset into a cleaned and structured format for further analysis. First, the code renames several columns for easier access using `janitor::clean_names` function. It filters the dataset to include only rows where `zone_type` is "response_button_image", representing the picture selected for that trial. Afterward, the function renames additional columns (`tlpic` to `TL`, `trpic` to `TR`, etc.). We also renamed `participant_private_id` to `subject`, `spreadsheet_row` to `trial`, and `reaction_time` to `RT`. This makes our columns consistent with the `edat` above for merging later on. Lastly, `reaction time` (RT) is converted to a numeric format for further numerical analysis.

It is important to note here that what the behavioral spreadsheet denotes as trial is not in fact the trial number used in the eye-tracking files. Thus it is imperative you use `spreadhseet row` as trial number to merge the two files.

```{r}
#|message: false
#|echo: true

eye_behav <- agg_eye_data %>%
 
  janitor::clean_names() %>%
 
  # Select specific columns to keep in the dataset
  dplyr::select(participant_private_id,  correct, tlpic, trpic, blpic, brpic, trialtype, targetword, tlcode, trcode, blcode, brcode, zone_name, zone_type,reaction_time, spreadsheet_row, trial_number,  response) %>%
 
  # Filter the rows where 'Zone.Type' equals "response_button_image"
  dplyr::filter(zone_type == "response_button_image") %>%
 
  # Rename columns for easier use and readability
  dplyr::rename(
    "TL" = "tlpic",              # Rename 'tlpic' to 'TL'
    "TR" = "trpic",             # Rename 'trpic' to 'TR'
    "BL" = "blpic",            # Rename 'blpic' to 'BL'
    "BR" = "brpic",                # Rename 'brpic' to 'BR'
    "targ_loc" = "zone_name",       # Rename 'Zone.Name' to 'targ_loc'
    "subject" = "participant_private_id",  # Rename 'Participant.Private.ID' to 'subject'
    "trial" = "spreadsheet_row",    # Rename 'spreadsheet_row' to 'trial'
    "acc" = "correct",              # Rename 'Correct' to 'acc' (accuracy)
    "RT" = "reaction_time"          # Rename 'Reaction.Time' to 'RT'
  ) %>%
 
  # Convert the 'RT' (Reaction Time) column to numeric type
  mutate(RT = as.numeric(RT),
         subject=as.factor(subject),
         trial=as.factor(trial))

```

### Audio onset

Because we are using spoken audio on each trial and running this experiment from the browser, audio onset is never going to to consistent across participants. In Gorilla there is an option to collect advanced audio features (you must make sure you select this when designing the study) such as when the audio play was requested, fired (played) and when the audio ended. We will want to incorporate this into our analysis pipeline. Gorilla records the onset of the audio which varies by participant. We are extracting that in the `audio_rt` object by filtering `zone_type` to `content_web_audio` and response equal to "AUDIO PLAY EVENT FIRED". This will tell us when the audio was triggered in the experiment. We are creating a column called (`RT_audio`) which we will use later on to correct for audio delays

```{r}
#| label: audio onset
#|
audio_rt <- agg_eye_data %>%
 
  janitor::clean_names()%>%

select(participant_private_id,zone_type, spreadsheet_row, reaction_time, response) %>%

  filter(zone_type=="content_web_audio", response=="AUDIO PLAY EVENT FIRED")%>%
  distinct() %>%
dplyr::rename("subject" = "participant_private_id",
       "trial" ="spreadsheet_row",  
       "RT_audio" = "reaction_time") %>%
select(-zone_type) %>%
mutate(RT_audio=as.numeric(RT_audio))
```

We then merge this information with `eye_behav`. We see that `RT_audio` has been added to our dataframe

```{r}

trial_data_rt <- merge(eye_behav, audio_rt, by=c("subject", "trial"))


```

### Trial removal

As stated above, participants who did not successfully calibrate 3 times or less were rejected from the experiment. Let's take a look at how many trials each participant had using the `eye_behav` file (see @tbl-part). Deciding to remove trials is ultimately up to the researcher. In our case, if participants have less than half the trials we will remove them from the experiment. In @tbl-badsubs we can see several participants failed some calibartion and do not have an adequate number of trials. Again we make no strong recommendations here. If you to decide to do this, we recommend pre-registration this decision.

```{r}

# find out how many trials each participant had
edatntrials <-trial_data_rt %>%
 dplyr::group_by(subject)%>%
 dplyr::summarise(ntrials=length(unique(trial)))
```

```{r}
#| label: tbl-part
#| tbl-cap: Trials by participant
#| echo: false

edatntrials %>%
  tt() %>%
  save_tt("edatntrials.png", overwrite = TRUE)

knitr::include_graphics("edatntrials.png")

```

```{r}
#| label: bad-subs
#| after-caption-space: 0pt


# get bad subs and remove them the analysis
edatntrials_bad <-trial_data_rt%>%
 dplyr::group_by(subject)%>%
 dplyr::summarise(ntrials=length(unique(trial))) %>%
  filter(ntrials < 75)

```

```{r}
#| label: tbl-badsubs
#| tbl-cap: Participants failing calibration
#| echo: false
#| after-caption-space: 0pt


edatntrials_bad %>%
  tt() %>%
  save_tt("edatntrials_bad.png", overwrite = TRUE)

knitr::include_graphics("edatntrials_bad.png")


```

Let's remove them from the analysis using the below code.

```{r}
#| label: remove bads

trial_data_rt <- trial_data_rt %>%
  filter(!subject %in% edatntrials_bad$subject)

```

### Practice trials

We do not want to include the practice trials in our experiment. The below target images refer to our practice trials. We filter these images out and save our `trial_data_sub` object as `trial_data_rt_filter`.

```{r}
trial_data_prac_filter <- trial_data_rt%>%
dplyr::filter(targetword != "cool.jpg",targetword != "bite.jpg",targetword != "milk.jpg", targetword != "willow.jpg")

```

### Low accuracy

In our experiment, we want to make sure accuracy is high. Again, we want participants that are fully attentive in the experiment. In the below code, we keep participants with accuracy equal to or above 80% and only include correct trials and save it to `trial_data_acc_clean`.

```{r}
#| label: accuracy

# Step 1: Calculate mean accuracy per subject and filter out subjects with mean accuracy < 0.8
subject_mean_acc <- trial_data_prac_filter %>%
  group_by(subject) %>%
  dplyr::summarise(mean_acc = mean(acc, na.rm = TRUE)) %>%
  filter(mean_acc > 0.8)

# Step 2: Join the mean accuracy back to the main dataset and exclude trials with accuracy < 0.8
trial_data_acc_clean <- trial_data_prac_filter %>%
  inner_join(subject_mean_acc, by = "subject") %>%
  filter(acc==1)

```

No participants scored below 80% on the VWP task. We will only look at correct trials.

### RTs

While removal of RTs is sometimes done, there is much debate on this We do not remove trials based on outlier RTs as there is much debate on this practice. If you would like to do this, we recommend the trimR package in R.

### Sampling Rate

While most commercial eye-trackers sample at a constant rate, data captured by webcams are widely inconsistent. Below is some code to calculate the sampling rate of each participant. Ideally, you should not have a sampling rate less than 5 Hz. It has been recommended you drop those values. The below function `analyze_sample_rate` calculates calculates the sampling rate for each subject and trial in our eye-tracking dataset (`edat`). It provides overall statistics, including the median and standard deviation of sampling rates in your experiment,and also generates a histogram of median sampling rates by subject. Looking at @fig-samprate, the sampling rate ranges from 5 to 30Hz with a median sampling rate of 22.5. This corresponds to previous webcam eye-tracking work.

```{r}
#| label: fig-samprate
#| fig-cap: Participant sampling-rate. A histogram and overlayed density plot shows median sampling rate by participant. the overall median and SD is highlighted in red.
#| after-caption-space: 0pt
#| echo: true

samp_rate <- analyze_sampling_rate(edat)
```

When using the above function, separate data frames are produced for participants and trials. These can be added to the behavioral dataframe using the below code.

```{r}

# Extract by-subject and by-trial sampling rates from the result
subject_sampling_rate <- samp_rate$median_SR_by_subject  # Sampling rate by subject
trial_sampling_rate <- samp_rate$SR_by_trial  # Sampling rate by trial
trial_sampling_rate$subject<-as.factor(trial_sampling_rate$subject)

# Assuming target_data is your other dataset that contains subject and trial information
# Append the by-subject sampling rate to target_data (based on subject)
subject_sampling_rate$subject <- as.factor(subject_sampling_rate$subject)

# Assuming target_data is your other dataset that contains subject and trial information
# Append the by-subject sampling rate to target_data (based on subject)
trial_sampling_rate$subject<-as.factor(trial_sampling_rate$subject)

# Assuming target_data is your other dataset that contains subject and trial information
# Append the by-subject sampling rate to target_data (based on subject)
subject_sampling_rate$subject <- as.factor(subject_sampling_rate$subject)
trial_data_acc_clean$subject <- as.factor(trial_data_acc_clean$subject)

target_data_with_subject_SR <- trial_data_acc_clean %>%
  left_join(subject_sampling_rate, by = "subject")

target_data_with_subject_SR$trial <- as.factor(target_data_with_subject_SR$trial)

# Append the by-trial sampling rate to target_data (based on subject and trial)
target_data_with_full_SR <- target_data_with_subject_SR %>%
  select(subject, trial, med_SR)%>%
  left_join(trial_sampling_rate, by = c("subject", "trial"))


```

We can add this to our main behavior dataframe `trial_data_acc_clean`.

```{r}

trial_data <- left_join(trial_data_acc_clean, target_data_with_full_SR, by=c("subject", "trial"))

```

Users can use the `filter_sampling_rate()` function to either either (1) throw out data, by subject, by trial, or both, and (2) label it sampling rates below a certain threshold as bad (TRUE or FALSE). Let's use the `filter_sampling_rate()` function to do this. We will use our `target_data_with_full_SR` object.

We leave it up to the user to decide what to do with low sampling rates and make no specific recommendations. In our case we are going to remove the data by-participant and by-trial (setting `action` = "both" ) if sampling frequency is below 5hz (`threshold`=5). The `filter_sampling_rate()` function is designed to process a dataset containing participant-level and trial-level sampling rates. It allows the user to either filter out data that falls below a certain sampling rate threshold or simply label it as “bad”. The function gives flexibility by allowing the threshold to be applied at the participant-level, trial-level, or both. It also lets the user decide whether to remove the data or flag it as below the threshold without removing it. If `action` = remove, the function will output how many subjects and trials were removed by on the threshold.

```{r}
filter_edat <- filter_sampling_rate(trial_data,threshold = 5,
                                         action = "remove",
                                         by = "both")

```

Here no subjects had a threshold below 5. However, 18 trials did, and they were removed.

### Out-of-bounds (outside of screen)

It is important that we do not include points that fall outside the standardized coordinates. The `gaze_oob()` function calculates how many of the data points fall outside the standardized range. Here we need our eye-tracking data (`edat`). Runing the `gaze_oob()` functon returns how many data points fall outside this range by-participant and also provides a percentage (see @tbl-oob). This information would be useful to include in the final paper. We can also add add by-participant and by-trial out of bounds data to our behavioral, trial-level, data (`filter_edat`) and finally exclude participants and trials with more than 30 missing data. The value of 30 is just a suggestion and should not be used as a rule of thumb for all studies nor are we endorsing this value.

```{r}
#| echo: true
#| results: hide
#| 
oob_data <- gaze_oob(edat)
```

```{r}
#| label: tbl-oob
#| tbl-cap: Out of bounds gaze statistics
#| echo: true
#| after-caption-space: 0pt


oob_data %>%
  tt() %>%
  save_tt("oob_data.png", overwrite=TRUE)


knitr::include_graphics("oob_data.png")


```

```{r}

remove_missing <- oob_data %>%                             # Start with the `oob_data` dataset and assign the result to `remove_missing`
  select(subject, total_missing_percentage) %>%            # Select only the `subject` and `total_missing_percentage` columns from `oob_data`
  left_join(filter_edat, by = "subject") %>%               # Perform a left join with `filter_edat` on the `subject` column, keeping all rows from `oob_data`
  filter(total_missing_percentage < 30)                    # Filter the data to keep only rows where `total_missing_percentage` is less than 30

```

As a note, we have decided to use an outer edge approach here (eliminating eye fixations that extend beyond the screen coordinates). @bramlett2024 have suggested an inner-edge approach and we may add this functionality once more testing is done. For now, we believe that the otter edge approach leads to the least amount of bias in the eye-tracking pipeline.

## Eye-tracking data

### Convergence and confidence

In the eye-tracking data we need to remove rows with poor convergence and confidence scores in our eye-tracking data. The `convergence` column refers to Webgazer's confidence in finding a face (and accurately predicting eye movements). Confidence values vary from 0 to 1, and numbers less than 0.5 suggest that the model has probably converged. `face_conf`represents the support vector machine (SVM) classifier score for the face model fit. This score indicates how strongly the image under the model resembles a face. Values vary from 0 to 1, and here numbers greater than 0.5 are indicative of a good model fit. In our `edat_1` object we filter out convergence less than 0.5 and face confidence greater than 0.5.

```{r}
edat_1 <- edat %>%
 dplyr::filter(convergence <= .5, face_conf >= .5) # remove poor convergnce and face confidence
```

### Combining eye and trial-level data

Next, we will combine the eye-tracking data and behavioral data using the left_join function. This function allows us to merge two datasets by matching rows based on a common key (or keys) that appear in both datasets. In this case, we’ll use left_join to add the behavioral data (to the eye-tracking data, preserving all rows from the eye-tracking data even if there isn’t a matching entry in the behavioral data (filled in with NA). This object is called `dat`. We use the `distinct()` function to remove duplicate rows in the dataset imposed by the inclusion of the NAs.

```{r}

dat <- left_join(edat_1, remove_missing, by = c("subject","trial"))

dat <- dat %>%
  distinct() # make sure to remove duplicate rows
```

## Areas of Interest

### Zone coordinates

In the lab, we can control every aspect of the experiment. Online we cant do this. Participants are going to be completing the experiment under a variety of conditions. This includes using different computers, with very different screen dimensions. To control for this, Gorilla outputs standardized zone coordinates (labled as `x_pred_normalised` and `y_pred_normalised` in the eye-tracking file) . As discussed in the Gorilla documentation, the Gorilla lays everything out in a 4:3 frame and makes that frame as big as possible. The normalized coordinates are then expressed relative to this frame; for example, the coordinate 0.5, 0.5 will always be the center of the screen, regardless of the size of the participant’s screen. We used the normalized coordinates in our analysis (in general, you should always use normalized coordinates) However, there are a few different ways to specify the four coordinates of the screen, which I think are worth highlighting here

#### Quadrant appraoch

One way is to make the AOIs as big as possible, dividing the screen into four quadrants. This approach has been used in multiple studies (e.g., [@Bramlett2024] ). In the coordinates for each quadrant is listed. shows how each quadrant looks in standardized space.

```{r}
#| echo: false
aoi_loc <- tibble(
  loc = c('TL', 'TR', 'BL', 'BR'),
  x_normalized = c(0, 0.5, 0, 0.5),
   y_normalized = c(0.5, 0.5, 0, 0),
  width_normalized = c(0.5, 0.5, 0.5, 0.5),
  height_normalized = c(0.5, 0.5, 0.5, 0.5)) %>%
  mutate(xmin = x_normalized, ymin = y_normalized,
         xmax = x_normalized+width_normalized,
         ymax = y_normalized+height_normalized)
```

```{r}
#| label: tbl-quadswrite
#| tbl-cap: Coordinates for quadrant appraoch
#| echo: false

aoi_loc %>%
  mutate(loc = case_when(
    loc == "TL" ~ "Top Left",
    loc == "TR" ~ "Top Right",
    loc == "BL" ~ "Bottom Left",
    loc == "BR" ~ "Bottom Right",
    TRUE ~ loc  # Keep other values as they are
  )) %>%
   dplyr:: rename("Location" = "loc") %>%
  tt()
```

```{r}
#| label: fig-quads
#| fig-cap: AOI coordiantes in standardized space using the quandrant appraoch
#| echo: false
#|

#look at the AOIs and see if they make sense

# Create a data frame for the quadrants
quadrants <- data.frame(
  x = c(0, 0.5, 0, 0.5),
  y = c(0.5, 0.5, 0, 0),
  width = c(0.5, 0.5, 0.5, 0.5),
  height = c(0.5, 0.5, 0.5, 0.5),
  color = c('red', 'blue', 'green', 'orange'),
  label = c('TL Width = 0.5', 'TR Width = 0.5', 'BL Width = 0.5', 'BR Width = 0.5')
)

# Create the plot
ggplot() +
  geom_rect(data = quadrants, aes(xmin = x, xmax = x + width, ymin = y, ymax = y + height, fill = color),
            color = 'black', alpha = 0) +
  geom_text(data = quadrants, aes(x = x + width/2, y = y + height/2, label = label), color = 'black', size = 5) +
  scale_fill_identity() +
  coord_fixed() +
  labs(x = 'Normalized X', y = 'Normalized Y', title = 'Quadrants with Width Annotations') +
  theme_minimal()

```

```{r}
#| label: fig-fixquads
#| fig-cap: All fixations in quadrants
#| echo: false

dat_colnames_temp <- dat %>%
  dplyr::filter(x_pred_normalised > 0, x_pred_normalised < 1, y_pred_normalised > 0, y_pred_normalised < 1)


sp <- ggplot(dat_colnames_temp, aes(x=x_pred_normalised, y=y_pred_normalised)) +
 
    geom_point(alpha=0.5) +
     # Annotate rectangles for the four quadrants with different colors
    annotate("rect", xmin=0, ymin=0,
             xmax=0.5, ymax=0.5, color="red", fill="lightblue", alpha=0.3) +
    annotate("rect", xmin=0.5, ymin=0,
             xmax=1, ymax=0.5, color="red", fill="lightgreen", alpha=0.3) +
    annotate("rect", xmin=0, ymin=0.5,
             xmax=0.5, ymax=1, color="red", fill="lightyellow", alpha=0.3) +
    annotate("rect", xmin=0.5, ymin=0.5,
             xmax=1, ymax=1, color="red", fill="lightpink", alpha=0.3) +
     
   
    theme_bw() +
    theme(axis.title.y=element_text(size = 14, face="bold"),
          axis.title.x = element_text(size=14, face="bold"),
          axis.text.x=element_text(size = 12, face="bold"),
          axis.text.y=element_text(size=12, face="bold"),
          legend.position = "bottom")
   
sp

```

We plot all the fixations in each of the quadrants highlighted in different colors (@fig-quads), removing points outside the standardized screen space.

##### Matching conditions with screen locations

In this experiment we have four different trial types:

1.  Target (T), Cohort (C) , Rhyme (R), Unrelated (U)
2.  Target (T), Cohort (C), Unrelated (U), Unrelated2 (U2)
3.  Target (T) , Unrelated (U), Unrelated2 (U2), Unrelated3 (U3)
4.  Target (T), Rhyme (R), Unrelated (U), Unrelated2(U2)

We need to will first match the pictures in the TL, TR, BL, BR columns to the correct trial condition code (i.e., T,C, R, U, U2, U3).

```{r}

# Assuming your data is in a data frame called dat
dat <- dat %>%
  mutate(
    Target = case_when(
      tlcode == "T" ~ TL,
      trcode == "T" ~ TR,
      blcode == "T" ~ BL,
      brcode == "T" ~ BR,
      TRUE ~ NA_character_  # Default to NA if no match
    ),
    Unrelated = case_when(
      tlcode == "U" ~ TL,
      trcode == "U" ~ TR,
      blcode == "U" ~ BL,
      brcode == "U" ~ BR,
      TRUE ~ NA_character_
    ),
    Unrelated2 = case_when(
      tlcode == "U2" ~ TL,
      trcode == "U2" ~ TR,
      blcode == "U2" ~ BL,
      brcode == "U2" ~ BR,
      TRUE ~ NA_character_
    ),
    Unrelated3 = case_when(
      tlcode == "U3" ~ TL,
      trcode == "U3" ~ TR,
      blcode == "U3" ~ BL,
      brcode == "U3" ~ BR,
      TRUE ~ NA_character_
    ),
    Rhyme = case_when(
      tlcode == "R" ~ TL,
      trcode == "R" ~ TR,
      blcode == "R" ~ BL,
      brcode == "R" ~ BR,
      TRUE ~ NA_character_
    ),
    Cohort = case_when(
      tlcode == "C" ~ TL,
      trcode == "C" ~ TR,
      blcode == "C" ~ BL,
      brcode == "C" ~ BR,
      TRUE ~ NA_character_
    )
  )


```

In our study, we need to track not only the condition of each image shown (such as Target, Cohort, Rhyme, or Unrelated) but also where each image is located on the screen during each trial as they are randomized on each trial. To do this, we use a function called `find_location()`. This function is designed to determine the location of a specific image on the screen by comparing the image with the list of possible locations.

The function `find_location()` first checks if the image is NA (missing). If the image is NA, the function returns NA, meaning that there's no location to find for this image. If the image is not NA, the function creates a vector called loc_names that lists the names of the possible locations. It then attempts to match the given image with the locations. If a match is found, it returns the name of the location (e.g., TL, TR, BL, or BR) of the image.

```{r}

# Apply the function to each of the targ, cohort, rhyme, and unrelated columns
dat_colnames <- dat %>%
  dplyr::rowwise() %>% # do this for every row
  mutate(
    targ_loc = find_location(c(TL, TR, BL, BR), Target),
    cohort_loc = find_location(c(TL, TR, BL, BR), Cohort),
    rhyme_loc = find_location(c(TL, TR, BL, BR), Rhyme),
    unrelated_loc = find_location(c(TL, TR, BL, BR), Unrelated),
    unrealted2_loc= find_location(c(TL, TR, BL, BR), Unrelated2),
    unrelated3_loc=find_location(c(TL, TR, BL, BR), Unrelated3)
  ) %>%
  ungroup()

```

Once we do this we can use `assign_aoi()` to loop through our object called `dat_colnames` and assign locations (i.e., TR, TL, BL, BR) to where participants looked at on the screen. This requires the `x` and `y` coordinates and the location of our aois `aoi_loc`. Here we are using the quadrant approach. This function will label non-looks and off screen coordinates with NA. To make it easier to read we change the numerals assigned by the function to actual screen locations.

```{r}

assign <- gazer::assign_aoi(dat_colnames,X="x_pred_normalised", Y="y_pred_normalised",aoi_loc = aoi_loc)


AOI <- assign %>%

  mutate(loc1 = case_when(

    AOI==1 ~ "TL",

    AOI==2 ~ "TR",

    AOI==3 ~ "BL",

    AOI==4 ~ "BR"

  ))
```

```{r}

AOI$target <- ifelse(AOI$loc1==AOI$targ_loc, 1, 0) # if in coordinates 1, if not 0.

AOI$unrelated <- ifelse(AOI$loc1 == AOI$unrelated_loc, 1, 0)# if in coordinates 1, if not 0.

AOI$unrelated2 <- ifelse(AOI$loc1 == AOI$unrealted2_loc, 1, 0)# if in coordinates 1, if not 0.

AOI$unrelated3 <- ifelse(AOI$loc1 == AOI$unrelated3_loc, 1, 0)# if in coordinates 1, if not 0.

AOI$rhyme <- ifelse(AOI$loc1 == AOI$rhyme_loc, 1, 0)# if in coordinates 1, if not 0.


AOI$cohort <- ifelse(AOI$loc1 == AOI$cohort_loc, 1, 0)# if in coordinates 1, if not 0.

```

The location of looks need to be "gathered" or pivoted to be in long format–-that is, from separate columns into a single column. This helps with visualization. We use `pivot_longer` which takes our columns (target, unrelated, unrealted2, unrelated3, rhyme, and cohort columns.) and them into a column called condition and place the values of 0 and 1 into a column called `Looks`.

```{r}

dat_long_aoi_me <- AOI  %>%
  select(subject, trial, trialtype, target, cohort, unrelated, unrelated2, unrelated3,  rhyme, time, x_pred_normalised, y_pred_normalised, RT_audio) %>%
    pivot_longer(
        cols = c(target, unrelated, unrelated2, unrelated3, rhyme, cohort),
        names_to = "condition",
        values_to = "Looks"
    )
```

To simply things, we will look at target, cohort, rhyme, and unrealted trial types which is stored in `dat_long_aoi_me_TCRU`. Using the `factor`() function we make the levels are in our preferred order. We also remove NAs from other trial types.

```{r}

dat_long_aoi_me_TCRU <- dat_long_aoi_me %>%
  filter(trialtype=="TCRU") %>% # get obly TCRU trials
  na.omit()
```

We can further clean up the data to improve its accuracy and relevance. Since there is a delay in audio presentation, captured as `RT_audio` in our dataset, we adjust the timing in the gaze_sub dataset by aligning time to the actual audio onset. To achieve this, we subtract `RT_audio` from time for each trial. In addition, we subtract 300 ms from this to account for the 100 ms of silence at the beginning of each audio clip and 200 ms to account for the oculomotor delay when planning an eye movement (Vivani, 1990). Additionally, we set our interest period between 0 ms (audio onset) and 2000 ms. we also filter out gaze coordinates that fall outside the standardized window, ensuring only valid data points are retained. The resulting dataset `gaze_sub_long` provides the proportion of looks to each Area of Interest (AOI), stored in the `meanfix` column. Researchers may also choose to downsample the data.

```{r}


gaze_sub_long <-dat_long_aoi_me_TCRU%>%
  group_by(subject, trial) %>%
  mutate(time = (time-RT_audio)-300) %>% # subtract audio rt onset for each filter
  filter(time >= 0, time <= 2000) %>%
  dplyr::filter(x_pred_normalised > 0, x_pred_normalised < 1, y_pred_normalised > 0, y_pred_normalised < 1)


```

## Downsampling

Downsampling into smaller time bins is a common practice in gaze data analysis, as it helps create a more manageable dataset and reduces noise. This is especially important in webcam eye-tracking where the sampling can be widely inconsistent between participants. Before your analysis, we recommend downsampling your data. In webgazeR we include the `downsample_gaze()` to assit with this. We apply this function to the `gaze_sub` object,and sety the `bin.length` argument to 100, which groups the data into 100-millisecond intervals. This adjustment means that each bin now represents a 100 ms passage of time. We specify time as the variable to base these bins on, allowing us to focus on broader patterns over time rather than individual millisecond fluctuations.

In addition, we indicate other variables for aggregation, such as condition, and use the newly created timebins variable, which represents the time intervals over which we aggregate data. There is no universal rule for selecting bin sizes, but Bramlett and Wiener found that differences in bin size did not significantly affect results as long as the sampling rate was above 5 Hz. The resulting downsampled dataset, output as @tbl-agg-sub, provides a simplified and more concise view of gaze patterns, making it easier to analyze and interpret broader trends.

```{r}
#| label: tbl-agg-sub
#| tbl-cap: Downsampled data

gaze_sub <- downsample_gaze(gaze_sub_long, bin.length=100, timevar="time", aggvars=c("condition", "time_bin"))
```

The above will not include subject. If you want to keep participant-level data we need to add subject to the `aggvars` argument (see @tbl-sub). If you do not plan to analyze proportion data, and instead what time binned data with binary outcomes preserved please set the `aggvars` argument to "none."

```{r}

# get back trial level data with no aggregation

gaze_sub_id <- downsample_gaze(gaze_sub_long, bin.length=100, timevar="time", aggvars="none")

```

```{r}
#| label: tbl-sub
#| tbl-cap: Timebinned data by participant
#| echo: false
#| 
head(gaze_sub_id)%>%
  tt()
```

## Visualizing time course data

To simplify plotting your time-course data, we have created the `plot_IA_proportions()` function. This function takes several arguments. The `ia_column` argument specifies the column containing your Interest Area (IA) labels. The `time_column` argument requires the name of your time bin column, and the `proportion_column` argument specifies the column containing fixation or look proportions. Additional arguments allow you to specify custom names for each IA in the `ia_mapping` argument, enabling you to label them as desired. As a note, you must use the `downsample_gaze()` function in order to use this plotting function.

Below we have plotted the time course data from our TCRU trials.

```{r}
#| label: fig-IATCRU
#| fig-cap: Time-course visualization for TCRU trials

tcru<-plot_IA_proportions(gaze_sub,
                    ia_column = "condition",
                    time_column = "timebins",
                    proportion_column = "Fix",
                    ia_mapping = list(target = "Target", cohort = "Cohort", rhyme = "Rhyme", unrelated = "Unrelated"))+
  ggtitle("TCRU") 

tcru
```

The objects produced by `plot_IA_proportions` are `ggplot` objects which means users can customize the figures how they seem fit.

```{r}
#| echo: false

dat_long_aoi_me_TUUU <- dat_long_aoi_me %>%
  filter(trialtype=="TUUU") %>%
  na.omit()

```

```{r}
#| echo: false

gaze_sub <-dat_long_aoi_me_TUUU%>% 
group_by(subject, trial) %>%
  mutate(time = (time-RT_audio)-300) %>% # subtract audio rt onset for each
 filter(time >= 0, time < 2000) %>% 
   dplyr::filter(x_pred_normalised > 0,
                x_pred_normalised < 1,
                y_pred_normalised > 0,
                y_pred_normalised < 1)


```

```{r}
#| echo: false

gaze_sub <- webgazeR::downsample_gaze(gaze_sub, bin.length=100, timevar="time", aggvars=c("condition", "time_bin"))


```

```{r}

#| echo: false

tuuu <- plot_IA_proportions(gaze_sub, 
                                        ia_column = "condition", 
                                         time_column = "timebins", 
                                         proportion_column = "Fix",
                             ia_mapping = list(target = "Target", 
                                         unrelated = "Unrelated", 
                                         unrelated2 = "Unrelated2", 
                                         unrelated3 = "Unrelated3"))+
  ggtitle("TUUU") 

```

```{r}
#| echo: false

dat_long_aoi_me_TCUU <- dat_long_aoi_me %>%
  filter(trialtype=="TCUU") %>%
  na.omit()

```

```{r}
#| echo: false


gaze_sub <-dat_long_aoi_me_TCUU%>% 
group_by(subject, trial) %>%
  mutate(time = (time-RT_audio)-100) %>% # subtract audio rt onset for each
 filter(time >= 0, time <= 2000) %>% 
   dplyr::filter(x_pred_normalised > 0,
                x_pred_normalised < 1,
                y_pred_normalised > 0,
                y_pred_normalised < 1)

```

```{r}
#| echo: false


gaze_sub <- webgazeR::downsample_gaze(gaze_sub, bin.length=100, timevar="time", aggvars=c("condition", "time_bin"))

```

```{r}
#| echo: false

tcuu <- plot_IA_proportions(gaze_sub, 
                                        ia_column = "condition", 
                                         time_column = "timebins", 
                                         proportion_column = "Fix", 
                                         ia_mapping = list(target = "Target",
                                         cohort = "Cohort", 
                                         unrelated = "Unrelated", 
                                         unrelated2 = "Unrelated2"))+
  ggtitle("TCUU")

```

```{r}
#| echo: false


dat_long_aoi_me_TRUU <- dat_long_aoi_me %>%
  filter(trialtype=="TRUU") %>%
  mutate(condition = factor(condition, levels = c("target", "rhyme", "unrelated", "unrelated2"))) %>% 
  na.omit()

```

```{r}
#| echo: false

gaze_sub <-dat_long_aoi_me_TRUU%>% 
group_by(subject, trial) %>%
  mutate(time = time-RT_audio) %>% # subtract audio rt onset for each
 filter(time >= 0, time <= 2000) %>% 
   dplyr::filter(x_pred_normalised > 0,
                x_pred_normalised < 1,
                y_pred_normalised > 0,
                y_pred_normalised < 1)

```

```{r}

#| echo: false

gaze_sub <- webgazeR::downsample_gaze(gaze_sub, bin.length=100, timevar="time", aggvars=c("condition", "time_bin"))


```

```{r}
#| echo: false

truu <- plot_IA_proportions(gaze_sub, 
                                        ia_column = "condition", 
                                         time_column = "timebins", 
                                         proportion_column = "Fix", 
                                         ia_mapping = list(target = "Target",
                                         rhyme = "Rhyme", 
                                         unrelated = "Unrelated", 
                                         unrelated2 = "Unrelated2")) + 
  ggtitle("TRUU")


```

We follow the same steps to get time course data for each trial type (see @fig-alltrials).

```{r}
#| label: fig-alltrials
#| fig-cap: Time course for each trial type in the VWP
#| echo: false
plot<- cowplot::plot_grid(tcru, truu, tuuu, tcuu)
ggsave("my_plot.png", plot = plot, width = 8, height = 6, units = "in")
knitr::include_graphics("my_plot.png")

```

#### Gorilla provided coordinates

Thus far, we have used the coordinates representing the four quadrants of the screen. However, Gorilla provides their own quadrants representing image location on the screen. To the authors' knowledge, these quadrants have not been looked at in any studies reporting eye-tracking results. Let's examine how reasonable our results are with these coordinates.

```{r}
# Get the list of all files in the folder
vwp_files  <- list.files(here::here("data", "monolinguals", "raw"), pattern = "\\.xlsx$", full.names = TRUE)

# Exclude files that contain "calibration" in their filename
vwp_paths_filtered <- vwp_files[!grepl("calibration", vwp_files)]

setwd(here::here("data", "monolinguals", "raw")) # set working directory to raw data folder

```

We will use the function `extract_aois` to get the standardized coordinates for each quadrant on screen. You can use the `zone_names` argument to get the zones you want to use. In our case we want the `TL`, `BR`, `BL` `TR` coordinates.

```{r}
# apply the extract_aois fucntion
aois <- extract_aois(vwp_paths_filtered, zone_names =  c("TL", "BR", "TR", "BL"))
```

```{r}
#| label: tbl-gorgaze
#| tbl-cap: Gorilla provided gaze coordinates
#| 
aois %>%
  tt()
```

```{r}
#| label: fig-gor
#| fig-cap: Gorilla provided standardized coordinates for the four qudrants on the screen
#| echo: false


#look at the AOIs and see if they make sense

# Create a data frame for the quadrants
quadrants <- tibble(
  x = aois$x_normalized,
  y = aois$y_normalized,
  width = aois$width_normalized,
  height = aois$height_normalized,
  color = c('red', 'blue', 'green', 'orange'),
  label = c('BL', 'TL', 'TR', 'BR')
)
# Create the plot
ggplot() +
  geom_rect(data = quadrants, aes(xmin = x, xmax = x + width, ymin = y, ymax = y + height, fill = color), 
            color = 'black', alpha = 0) +
  geom_text(data = quadrants, aes(x = x + width/2, y = y + height/2, label = label), color = 'black', size = 5) +
  scale_fill_identity() +
  coord_fixed() +
  labs(x = 'Normalized X', y = 'Normalized Y', title = 'Quadrants with Width Annotations') +
  theme_minimal()

```

We can take these coordinates and use them in our analysis. Looking at @fig-gor the AOIs are a lot smaller than the quadrant approach.

Below we use these coordinates in our `assign_aoi` function by passing it to `aoi_loc` and proceed like we did before.

```{r}



assign <- webgazeR::assign_aoi(dat_colnames,X="x_pred_normalised", Y="y_pred_normalised",aoi_loc = aois)


AOI <- assign %>%

  mutate(loc1 = case_when(

    AOI==1 ~ "BL", 

    AOI==2 ~ "TL", 

    AOI==3 ~ "TR", 

    AOI==4 ~ "BR"

  ))

```

```{r}
#| echo: false
#| 
AOI$target <- ifelse(AOI$loc1==AOI$targ_loc, 1, 0) # if in coordinates 1, if not 0. 

AOI$unrelated <- ifelse(AOI$loc1 == AOI$unrelated_loc, 1, 0)# if in coordinates 1, if not 0. 

AOI$unrelated2 <- ifelse(AOI$loc1 == AOI$unrealted2_loc, 1, 0)# if in coordinates 1, if not 0. 

AOI$unrelated3 <- ifelse(AOI$loc1 == AOI$unrelated3_loc, 1, 0)# if in coordinates 1, if not 0. 

AOI$rhyme <- ifelse(AOI$loc1 == AOI$rhyme_loc, 1, 0)# if in coordinates 1, if not 0. 


AOI$cohort <- ifelse(AOI$loc1 == AOI$cohort_loc, 1, 0)# if in coordinates 1, if not 0. 

head(AOI)
```

```{r}
#| echo: false
#| 

dat_long_aoi_me <- AOI  %>%
  select(subject, trial, trialtype, target, cohort, unrelated, unrelated2, unrelated3,  rhyme, time, x_pred_normalised, y_pred_normalised, RT_audio) %>%
    pivot_longer(
        cols = c(target, unrelated, unrelated2, unrelated3, rhyme, cohort),
        names_to = "condition",
        values_to = "Looks"
    )

```

```{r}
#| echo: false
#| 
dat_long_aoi_me_TCRU <- dat_long_aoi_me %>%
  filter(trialtype=="TCRU") %>% # get obly TCRU trials %>% 
  na.omit() 

```

```{r}
#| echo: false
#| 
gaze_sub <-dat_long_aoi_me_TCRU%>% 
group_by(subject, trial) %>%
  mutate(time = (time-RT_audio)-300) %>% # subtract audio rt onset for each
 filter(time >= -200, time <= 2000) %>% 
   dplyr::filter(x_pred_normalised > 0,
                x_pred_normalised < 1,
                y_pred_normalised > 0,
                y_pred_normalised < 1)

```

```{r}
#| echo: false
#| 
gaze_sub <- downsample_gaze(gaze_sub, bin.length=100, timevar="time", aggvars=c("condition", "time_bin"))

```

##### Visualizing time-course

The results using the Gorilla coordinates are promising. In @fig-tcru-gor shows a larger cohort effect. This suggests that these coordinates may hold significant potential for research applications. However, further investigation is needed to confirm their reliability and applicability.

```{r}
#| label: fig-tcru-gor
#| fig-cap: Time-coruse visualization for Gorilla provided coordinates
#| echo: false


tcru<-plot_IA_proportions(gaze_sub, 
                    ia_column = "condition", 
                    time_column = "time_bin", 
                    proportion_column = "Fix", 
                  ia_mapping = list(target = "Target", 
                    cohort = "Cohort", 
                     rhyme = "Rhyme", 
                      unrelated = "Unrelated")) + 
  ggtitle("TCRU") 


tcru

```

## Modeling Data

When analyzing VWP data there are many analytic approaches to choose from (e.g., growth curve analysis (GCA), cluster permutation tests (CPT), generalized additive mixed models (GAMMS), logistic multilevel models, divergent point analysis, etc.), and a lot has already been written describing these methods and applying them to visual world fixation data (see Stone et al, 2020; Ito et al.2023; McMurray et al., 2024;), including webcam fixation data (@brmlett). This tutorial's goal, however, is to not evaluate analytic approaches, or proselytize for a specific set of methods. All methods have there strengths and weaknesses (see Ito et al., 2023). Nevertheless, statistical modeling is not divorced from the questions researchers using the VWP have and thus serious thought needs to be given to the proper analysis. In the VWP, there are two general questions one might be interested in: (1) Are there any overall difference in fixations between conditions and (2) Are there any time course differences in fixations between conditions. One question we might want to answer is if there are any fixation differences between the cohort and unrelated conditions across the time course. --highlight the cohort or competitor effect. One reasonable apporach here is to conduct a cluster permutation analysis.

## CPT

Permuation cluster analysis is a technique that is becoming increasingly popular, especially in the cognitvie neuropsychology domain to analyze MEG-EEG (Maris & Oostenveld, 2007). While there exists a number of cluster analysis functions in programs like MNE-Python (Gramfort et al., 2014) and Matlab’s FieldTrip (Oostenveld et al., 2011), what I want to show you is how you can do this analysis with visual world eye-tracking data in R. The implementation of CPT has not been widely used in the VWP, but is slowly becoming mroe widely used (see Ito et al. 2023; Haung & XXXX)

Before I show you how to apply this method, I want to briefly explain the Cluster-Based Permutation Test (CPT) method. The CPT is a data-driven approach that increases statistical power while controlling for Type I errors across multiple comparisons—exactly what we need when analyzing fixations across the time course!

The clustering procedure involves three main steps:

1.  Cluster Formation: Dependent-sample t-tests are conducted for every data point (condition by time). Adjacent data points that surpass the mass-univariate significance threshold (p \< .05) are combined into clusters. The cluster-level statistic is computed as the sum of the t-values (or F-values) within each cluster.

2.  Null Distribution Creation: A surrogate null distribution is generated by randomly permuting the conditions within subjects. This randomization is repeated 𝑛 n times, and the cluster-level statistic is computed for each permutation.

3.  Significance Testing: The cluster-level statistic from the observed (real) comparison is compared to the null distribution. Clusters with statistics falling in the highest or lowest 2.5% of the null distribution are considered significant.

A benefit of this approach is that one does not need to correct for multiple compairsons

We will use the `permutaes` and `permuco` packages in R and the function to run a multilevel cluster permutation model with our `gaze_sub_id` dataset where each row in `Looks` denotes whether the Area of Interest (IA) was fixated, with values of zero (not fixated) or one (fixated). We will apply the permutes.glmer function to look at the within and cross competetion effects.

Below is sample code to perform GCA analysis in R using the `permutaes.glmer()` function and the `glmer` function from `lme4` package.

### Multilevel Cluster Permutation Analysis

Below is sample code to perform GCA analysis in R using the `code_poly()` function and the `glmer` function from `lme4` package.

in @tbl-gca, we see the quadratic by condition interaction term is significant. This indicates the fixation curvature differed between the cohort and unrelated codnitions in the peak, with cohorts peaking early. This provides evidence that webcam eye-tracking can detect competition effects in L1 processing.

# Experiment 2: L2 Spanish Speakers

TThe next example dataset examines the competitive dynamics of L2 learners of Spanish. Specifically, we investigated within-language and cross-language competition using webcam-based eye-tracking. A recent study by McCall et al. explored within- and cross-linguistic competition in adult L2 learners using a cross-linguistic Visual World Paradigm (VWP). Their task included two key conditions:

1.  **Spanish-Spanish condition**: A Spanish competitor was presented alongside the target word. For example, if the target word spoken was "cielo" (sky), the Spanish competitor was "ciencia" (science).

2.  **Spanish-English (cross-linguistic) condition**: An English competitor was presented for the Spanish target word. For example, if the target word spoken was "botas" (boots), the English competitor was "border."

McCall et al. observed competition effects in both conditions: within-Spanish competition (e.g., *cielo* - *ciencia*) and cross-linguistic competition (e.g., *botas* - *border*). For this tutorial, we collected data to replicate their pattern of findings.

There are two key differences between our dataset and the original study by McCall et al.:

1.  McCall et al. focused on adult L2 Spanish speakers and posed more fine-grained questions about the time course of competition and resolution.

2.  Unlike McCall et al., who measured Spanish proficiency objectively (e.g., using LexTALE-esp; Izura et al., 2014), we relied on Prolific’s filters to recruit L2 Spanish speakers.

Our primary goal was to demonstrate the processing steps required to analyze webcam-based eye-tracking data. A secondary goal was to provide evidence of L2 competition using this methodology, with a focus on the preprocessing steps necessary for researchers to analyze their data effectively. This foundational step enables researchers to investigate more detailed questions about L2 processing using webcam-based eye-tracking.

## Method

All tasks for this experiment can be previewed here (<https://app.gorilla.sc/openmaterials/917915>).

### Participants

A total of 187 participants consented to participate in the study using the Gorilla hosting and experiment platform. Of these, 111 passed the headphone screener checkpoint and proceeded to the task. Among them, 32 participants successfully completed the Visual World Paradigm (VWP) task with at least 100 trials, while 79 participants failed calibration. Ninety-one participants completed the entire experiment, including the final questionnaires. \@tbl-demo2 provides basic demographic information about the participants who completed the full experiment. After applying additional exclusion criteria (low accuracy ( \< 80%) and excessive missing eye-data (\> 30%) , the final sample consisted of 28 participants.

```{r}
#| label: tbl-demo2
#| tbl-cap: Demographic variables Experiment 2
#| echo: false
#| after-caption-space: 0pt


quest1 <- read_csv(here::here("data", "L2", "data_exp_196386-v5_questionnaire-3956.csv"))
quest2 <- read_csv(here::here("data", "L2", "data_exp_196386-v6_questionnaire-3956.csv"))
quest3 <- read_csv(here::here("data", "L2", "data_exp_196386-v5_questionnaire-w1i9.csv"))
quest4 <- read_csv(here::here("data", "L2", "data_exp_196386-v6_questionnaire-w1i9.csv"))

quest <- rbind(quest1, quest2, quest3, quest4)

quest %>%
    select(`Participant Private ID`, `Question Key`, Response, `Participant Browser`) %>%
    drop_na() %>%
    filter(
        !`Question Key` %in% c("END QUESTIONNAIRE", "BEGIN QUESTIONNAIRE") &
            !str_detect(`Question Key`, "quantised")
    ) %>%
    pivot_wider(names_from = `Question Key`, values_from = Response) %>%
    rename(
        Age = `response-1`,
        Gender = `response-2`,
     `Spoken dialect` = `response-3`,
        Race = `response-4`,
     Browser=`Participant Browser`, 
     `Years Speaking Spanish` =`response-7`, 
     `Age Learned` = `response-6`, 
     `Percentage Spanish` = `response-8`
    ) %>%
  mutate(Age = as.numeric(Age),
    `Percentage Spanish` = as.numeric(gsub("%", "", `Percentage Spanish`)), 
      Browser = gsub("[0-9]", "", Browser), 
      Browser= gsub("\\.+$", "", Browser), 
    `Years Speaking Spanish` = gsub(" .*", "", `Years Speaking Spanish`), # Extract numeric part before any spaces
    `Years Speaking Spanish` = gsub("[^0-9.]", "",`Years Speaking Spanish`),     # Remove non-numeric characters
    `Years Speaking Spanish` = case_when(                             # Map special cases to 0
      `Years Speaking Spanish` %in% c("", "N/A", "Zero", "I do not use spanish") ~ "0",
      TRUE ~ `Years Speaking Spanish`
    ),
    `Years Speaking Spanish` = as.numeric(`Years Speaking Spanish`)              # Convert to numeric
  ) %>%  
    select(Age, Gender, `Spoken dialect`, Race, Browser, `Years Speaking Spanish`, `Percentage Spanish`) %>%
  tbl_summary(statistic = list(
      Age ~ "({min}, {max}), {mean}({sd})", 
      `Percentage Spanish` ~ "{mean}({sd})", 
      `Years Speaking Spanish` ~ "({min}, {max}), {mean}({sd})", 
      all_categorical() ~ "{n} / {N} ({p}%)"
    ),) %>%
   bold_labels() %>%
  as_gt() %>%
    gt::gtsave( # save table as image
    filename = "my_demo2.png"
  )


knitr::include_graphics("my_demo2.png")

```

### Materials

We used the same tasks as Experiment 1, with exceptions noted below.

#### VWP

##### Design and items

For Experiment 2 we adapted materials from MCcall et al (2022). In their cross-lingusitic VWP, Participants were presented with four pictures and a spoken Spanish word, selecting the matching image by clicking on it. The word stimuli were chosen from textbooks used by students in their first and second year college Spanish courses.

The item sets consisted of two types of phonologically-related word pairs: one pair of Spanish-Spanish words and another of Spanish-English words. The Spanish-Spanish pairs were unrelated to the Spanish-English pairs. All the word pairs were carefully controlled on a number of dimensions (see McCall et al.) making it an excellent set of materials to use.

There were three experimental conditions: (1) the Spanish-Spanish condition, where one of the Spanish words was the target and the other was the competitor; (2) the Spanish-English condition, where a Spanish word was the target and its English phonological cohort served as the competitor; and (3) the No Competitor condition, where the Spanish word did not overlap with any other word in the set. The Spanish-Spanish condition had twice as many trials as the other conditions due to the interchangeable nature of the target and competitor words in that pair.

There were 15 sets of 4 items (this was half the number of sets used in McCall et al.). Each item within a set was repeated 4 times as the target word. This yielded 240 trials (15 sets × 4 items per set × 4 repetitions). Each item set consisted of one Spanish-Spanish cohort pair and one Spanish-English cohort pair. Both items in a Spanish-Spanish pair had a“reciprocal” competitor relationship (that is, we could test activation for cielo given ciencia, and for cienciagiven cielo). Consequently, there were 120 trials in the Spanish-Spanish condition. In contrast, only one itemo f the Spanish-English pair had the speciﬁed competitor relationship (we could test activation for frontera “border”, given botas, but when hearing frontera, therewas no competitor). Thus, there were only 60 trials for each the Spanish-English competition as well as the No Competitor condition. Items occurred in each of the four corners of the screen on an equal numbers of trials.

#### Demographics questionnaire

In addition to the questions we asked in Experiment 1, we probed L2 speaking ability. We asked participants when they started speaking Spanish, how many years of Spanish speaking, and to provide a percentage of time Spanish is spoken in their daily lives.

## Pre-processing

We will follow the same pre-processing steps as we did above. For the sake of brevity we will output the code chunks and only go in detail when steps differ from above

```{r}
#| label: L2 trial data 
# load in trial level data 

# combine data from version 5 and 6 of the task
L2_1 <- read_csv(here("data", "L2", "data_exp_196386-v6_task-scf6.csv"))
L2_2 <- read_csv(here("data", "L2", "data_exp_196386-v5_task-scf6.csv"))

L2_data <- rbind(L2_1, L2_2)

```

```{r}
#| label: eye data L2
# Get the list of all files in the folder
vwp_files_L2  <- list.files(here::here("data", "L2", "raw"), pattern = "\\.xlsx$", full.names = TRUE)

# Exclude files that contain "calibration" in their filename
vwp_paths_filtered_L2 <- vwp_files_L2[!grepl("calibration", vwp_files_L2)]

```

```{r}
#| label: edat L2
setwd(here::here("data", "L2", "raw")) # set working directory to raw data folder

edat_L2 <- merge_webcam_files(vwp_paths_filtered_L2, screen_index=4) # eye tracking occured ons creen index 4

```

```{r}
#|message: false
#|echo: true

eye_behav_L2 <- L2_data %>%
 
  janitor::clean_names() %>%
 
  # Select specific columns to keep in the dataset
  dplyr::select(participant_private_id,  correct, tlpic, trpic, blpic, brpic, condition, eng_targetword, targetword, typetl, typetr, typebl, typebr, zone_name, zone_type,reaction_time, spreadsheet_row,  response) %>%
 
  # Filter the rows where 'Zone.Type' equals "response_button_image"
  dplyr::filter(zone_type == "response_button_image") %>%
 
  # Rename columns for easier use and readability
  dplyr::rename(
    "TL" = "tlpic",              # Rename 'tlpic' to 'TL'
    "TR" = "trpic",             # Rename 'trpic' to 'TR'
    "BL" = "blpic",            # Rename 'blpic' to 'BL'
    "BR" = "brpic",                # Rename 'brpic' to 'BR'
    "targ_loc" = "zone_name",       # Rename 'Zone.Name' to 'targ_loc'
    "subject" = "participant_private_id",  # Rename 'Participant.Private.ID' to 'subject'
    "trial" = "spreadsheet_row",    # Rename 'spreadsheet_row' to 'trial'
    "acc" = "correct",              # Rename 'Correct' to 'acc' (accuracy)
    "RT" = "reaction_time"          # Rename 'Reaction.Time' to 'RT'
  ) %>%
 
  # Convert the 'RT' (Reaction Time) column to numeric type
  mutate(RT = as.numeric(RT),
         subject=as.factor(subject),
         trial=as.factor(trial))

```

```{r}
#| label: audio onset L2
#|
audio_rt_L2 <- L2_data %>%
 
  janitor::clean_names()%>%

select(participant_private_id,zone_type, spreadsheet_row, reaction_time, response) %>%

  filter(zone_type=="content_web_audio", response=="AUDIO PLAY EVENT FIRED")%>%
  distinct() %>%
dplyr::rename("subject" = "participant_private_id",
       "trial" ="spreadsheet_row",  
       "RT_audio" = "reaction_time") %>%
select(-zone_type) %>%
mutate(RT_audio=as.numeric(RT_audio))

```

```{r}
#| label: L2-merge-audio

trial_data_rt_L2 <- merge(eye_behav_L2, audio_rt_L2, by=c("subject", "trial"))

```

```{r}
# find out how many trials each participant had
edatntrials_L2 <-trial_data_rt_L2 %>%
 dplyr::group_by(subject)%>%
 dplyr::summarise(ntrials=length(unique(trial)))
```

```{r}
#| label: tbl-part-L2
#| tbl-cap: Trials by participant Experiment 2
#| echo: false

edatntrials_L2 %>%
  tt() %>%
  save_tt("edatntrialsL2.png", overwrite = TRUE)

knitr::include_graphics("edatntrialsL2.png")

```

```{R}
#| label: bad-subs-L2


# get bad subs and remove them the analysis

edatntrials_bad_L2 <-trial_data_rt_L2 %>%
 dplyr::group_by(subject)%>%
 dplyr::summarise(ntrials=length(unique(trial))) %>%
  dplyr::filter(ntrials > 100)
  
```

```{r}
#| label: remove bads L2

trial_data_rt_L2 <- trial_data_rt_L2 %>%
  filter(subject %in% edatntrials_bad_L2$subject)

```

```{r}
#| label: accuracy-L2

# Step 1: Calculate mean accuracy per subject and filter out subjects with mean accuracy < 0.8
subject_mean_acc_L2 <- trial_data_rt_L2 %>%
  group_by(subject) %>%
  dplyr::summarise(mean_acc = mean(acc, na.rm = TRUE)) %>%
  filter(mean_acc > 0.8)

# Step 2: Join the mean accuracy back to the main dataset and exclude trials with accuracy < 0.8
trial_data_acc_clean_L2 <- trial_data_rt_L2 %>%
  inner_join(subject_mean_acc_L2, by = "subject") %>%
  filter(acc==1) # only use accurate responses for fixation analysis

```

```{r}
#| label: fig-samprate-L2
#| fig-cap: Participant sampling-rate for L2 experiment. A histogram and overlayed density plot shows median sampling rate by participant. the overall median and SD is highlighted in red.
#| after-caption-space: 0pt
#| echo: true

samp_rate <- analyze_sampling_rate(edat_L2)
```

```{r}
# Extract by-subject and by-trial sampling rates from the result
subject_sampling_rate_L2 <- samp_rate$median_SR_by_subject  # Sampling rate by subject
trial_sampling_rate_L2 <- samp_rate$SR_by_trial  # Sampling rate by trial
trial_sampling_rate_L2$subject<-as.factor(trial_sampling_rate_L2$subject)

# Assuming target_data is your other dataset that contains subject and trial information
# Append the by-subject sampling rate to target_data (based on subject)
subject_sampling_rate_L2$subject <- as.factor(subject_sampling_rate_L2$subject)

# Assuming target_data is your other dataset that contains subject and trial information
# Append the by-subject sampling rate to target_data (based on subject)
trial_sampling_rate_L2$subject<-as.factor(trial_sampling_rate_L2$subject)

# Assuming target_data is your other dataset that contains subject and trial information
# Append the by-subject sampling rate to target_data (based on subject)
subject_sampling_rate_L2$subject <- as.factor(subject_sampling_rate_L2$subject)
trial_data_acc_clean_L2$subject <- as.factor(trial_data_acc_clean_L2$subject)

target_data_with_subject_SR_L2 <- trial_data_acc_clean_L2 %>%
  left_join(subject_sampling_rate_L2, by = "subject")

target_data_with_subject_SR_L2$trial <- as.factor(target_data_with_subject_SR_L2$trial)

# Append the by-trial sampling rate to target_data (based on subject and trial)
target_data_with_full_SR_L2 <- target_data_with_subject_SR_L2 %>%
  select(subject, trial, med_SR)%>%
  full_join(trial_sampling_rate_L2, by = c("subject", "trial"))

```

```{r}

trial_data_L2 <- left_join(trial_data_acc_clean_L2, target_data_with_full_SR_L2, by=c("subject", "trial"))

```

```{r}
filter_edat_L2 <- filter_sampling_rate(trial_data_L2,threshold = 5,
                                         action = "remove",
                                         by = "both")
```

Here 1 subject is removed along with 107 trials from that participant.

## Out-of-bounds (outside of screen)

```{r}
#| echo: true
#| results: hide
#| 
oob_data_L2 <- gaze_oob(edat_L2)
```

```{r}
#| label: tbl-oob-L2
#| tbl-cap: Out of bounds gaze statistics
#| echo: true
#| after-caption-space: 0pt


oob_data %>%
  tt() %>%
  save_tt("oob_data_L2.png", overwrite=TRUE)


knitr::include_graphics("oob_data_L2.png")


```

```{r}

remove_missing <- oob_data_L2 %>%                             # Start with the `oob_data` dataset and assign the result to `remove_missing`
  select(subject, total_missing_percentage) %>%            # Select only the `subject` and `total_missing_percentage` columns from `oob_data`
  left_join(filter_edat_L2, by = "subject") %>%               # Perform a left join with `filter_edat` on the `subject` column, keeping all rows from `oob_data`
  filter(total_missing_percentage < 30)  %>%                   # Filter the data to keep only rows where `total_missing_percentage` is less than 30 %>%
na.omit()

```

## Eye-tracking data

### Convergence and confidence

```{r}
edat_1_L2 <- edat_L2 %>%
 dplyr::filter(convergence <= .5, face_conf >= .5) # remove poor convergnce and face confidence
```

### Combining eye and trial-level data

```{r}

dat_L2 <- right_join(edat_1_L2,remove_missing,  by = c("subject","trial"))

dat_L2 <- dat_L2 %>%
  distinct() # make sure to remove duplicate rows %>%

```

## Areas of Interest

```{r}
#| label: quads-L2
#| tbl-cap: Coordinates for quadrant appraoch
#| echo: false
# Create a data frame for the quadrants with an added column for the quadrant labels
aoi_loc <- data.frame(
  loc = c('TL', 'TR', 'BL', 'BR'), 
  x_normalized = c(0, 0.5, 0, 0.5),
   y_normalized = c(0.5, 0.5, 0, 0),
  width_normalized = c(0.5, 0.5, 0.5, 0.5),
  height_normalized = c(0.5, 0.5, 0.5, 0.5)) %>% 
  
  mutate(xmin = x_normalized, ymin = y_normalized,
         xmax = x_normalized+width_normalized,
         ymax = y_normalized+height_normalized)
```

```{r}

# Assuming your data is in a data frame called dat
dat_L2 <- dat_L2 %>%
  mutate(
    Target = case_when(
      typetl == "target" ~ TL,
      typetr == "target" ~ TR,
      typebl == "target" ~ BL,
      typebr == "target" ~ BR,
      TRUE ~ NA_character_  # Default to NA if no match
    ),
    Unrelated = case_when(
      typetl == "unrelated1" ~ TL,
      typetr == "unrelated1" ~ TR,
      typebl == "unrelated1" ~ BL,
      typebr == "unrelated1" ~ BR,
      TRUE ~ NA_character_
    ),
    Unrelated2 = case_when(
      typetl == "unrelated2" ~ TL,
      typetr == "unrelated2" ~ TR,
      typebl == "unrelated2" ~ BL,
      typebr == "unrelated2" ~ BR,
      TRUE ~ NA_character_
    ),
    Cohort = case_when(
      typetl == "cohort" ~ TL,
      typetr == "cohort" ~ TR,
      typebl == "cohort" ~ BL,
      typebr == "cohort" ~ BR,
      TRUE ~ NA_character_
    )
  )


```

```{r}

# Apply the function to each of the targ, cohort, rhyme, and unrelated columns
dat_colnames_L2 <- dat_L2 %>%
  rowwise() %>%
  mutate(
    targ_loc = find_location(c(TL, TR, BL, BR), Target),
    cohort_loc = find_location(c(TL, TR, BL, BR), Cohort),
    unrelated_loc = find_location(c(TL, TR, BL, BR), Unrelated), 
    unrealted2_loc= find_location(c(TL, TR, BL, BR), Unrelated2), 
  ) %>%
  ungroup()

```

```{r}

assign_L2 <- gazer::assign_aoi(dat_colnames_L2,X="x_pred_normalised", Y="y_pred_normalised",aoi_loc = aoi_loc)


AOI_L2 <- assign_L2 %>%

  mutate(loc1 = case_when(

    AOI==1 ~ "TL", 

    AOI==2 ~ "TR", 

    AOI==3 ~ "BL", 

    AOI==4 ~ "BR"

  ))
```

```{r}
AOI_L2$target <- ifelse(AOI_L2$loc1==AOI_L2$targ_loc, 1, 0) # if in coordinates 1, if not 0. 

AOI_L2$unrelated <- ifelse(AOI_L2$loc1 == AOI_L2$unrelated_loc, 1, 0)# if in coordinates 1, if not 0. 

AOI_L2$unrelated2 <- ifelse(AOI_L2$loc1 == AOI_L2$unrealted2_loc, 1, 0)# if in coordinates 1, if not 0. 

AOI_L2$cohort <- ifelse(AOI_L2$loc1 == AOI_L2$cohort_loc, 1, 0)# if in coordinates 1, if not 0. 

```

```{r}

dat_long_aoi_me_L2 <- AOI_L2  %>%
  select(subject, trial, condition, target, cohort, unrelated, unrelated2, time, x_pred_normalised, y_pred_normalised, RT_audio) %>%
    pivot_longer(
        cols = c(target, unrelated, unrelated2,cohort),
        names_to = "condition1",
        values_to = "Looks"
    ) %>%
  mutate(condition1 = ifelse(condition1=="unrelated2", "unrelated", condition1))

```

### Samples to bins

```{r}
# repalce the numbers appended to conditions that somehow got added 
dat_long_aoi_me_comp <- dat_long_aoi_me_L2 %>%
  mutate(condition = str_replace(condition, "TCUU-SPENG\\d*", "TCUU-SPENG")) %>%
  mutate(condition = str_replace(condition, "TCUU-SPSP\\d*", "TCUU-SPSP"))%>% 
  na.omit() 

```

```{r}

gaze_sub_L2_long <-dat_long_aoi_me_comp%>% 
group_by(subject, trial, condition) %>%
  mutate(time = (time-RT_audio)-300) %>% # subtract audio rt onset and account for occ motor planning and silence in audio
 filter(time >= -200, time < 2000) %>% 
   dplyr::filter(x_pred_normalised > 0,
                x_pred_normalised < 1,
                y_pred_normalised > 0,
                y_pred_normalised < 1)

```

### Downsampling

```{r}

gaze_sub_L2 <- downsample_gaze(gaze_sub_L2, bin.length=100, timevar="time", aggvars=c("condition", "condition1", "time_bin"))

```

```{r}
#| label: fig-L2comp
#| 
#| fig-width: 12
#| fig-height: 10

tcru <- plot_IA_proportions(
    gaze_sub_L2, 
    ia_column = "condition1",
    time_column = "time_bin",
    proportion_column = "Fix",
    condition_column = "condition", 
    ia_mapping = list(target = "Target", cohort = "Cohort", unrelated = "Unrelated")
) + 
    facet_wrap(
        ~ condition,             # Facet by condition
        labeller = labeller(
            condition = c(
                "TCUU-ENGSP" = "No Competitor",
                "TCUU-SPENG" = "Spanish-English",
                "TCUU-SPSP" = "Spanish-Spanish"
            )
        ),
        ncol = 2                # Specify 2 columns to achieve 2 on top row, 1 on bottom
    ) +
    labs(tag = "a") +         # Optional tag for labeling
    theme(
        strip.text = element_text(size = 14),  # Adjust facet label text size
        panel.spacing = unit(1, "lines")       # Add space between panels
    )

# Print the plot
tcru

```

## Analysis

```{r}
gaze_sub_L2_long <-dat_long_aoi_me_comp%>% 
group_by(subject, trial, condition) %>%
  mutate(time = (time-RT_audio)-300) %>% # subtract audio rt onset and account for occ motor planning and silence in audio
 filter(time >= -200, time < 2000) %>% 
   dplyr::filter(x_pred_normalised > 0,
                x_pred_normalised < 1,
                y_pred_normalised > 0,
                y_pred_normalised < 1)
```

```{r}

gaze_sub_L2 <- downsample_gaze(gaze_sub_L2_long, bin.length=100, timevar="time", aggvars=c("none"))

```

```{r}

gaze_sub_L2_cp <- gaze_sub_L2 %>%
  filter(condition=="TCUU-SPSP", condition1=="cohort" | condition1=="unrelated") %>%
  mutate(condition1_code=ifelse(condition1=="cohort", 0.5, -0.5))

```

```{r}

library(permutes)
library(permuco)
library(foreach) # for par 

cpa.lme = permutes::clusterperm.glmer(Looks~ condition1_code + (1|subject) + (1|trial), data=gaze_sub_L2_cp, series.var=~time_bin, nperm = 1000, parallel=TRUE)

```

```{r}
sig.clusters = cpa.lme %>% filter(Factor=='condition1_code', !is.na(cluster_mass), p.cluster_mass<.05) %>% 
    mutate_at(vars(time_bin), as.numeric) %>% group_by(cluster) %>% 
    summarise(cluster_mass=min(cluster_mass), p.cluster_mass=min(p.cluster_mass), bin_start=min(time_bin), bin_end=max(time_bin), t=mean(t)) %>% 
    mutate(sign=ifelse(t<0,-1,1), time_start=(bin_start-2)*100, time_end=(bin_end-2)*100) %>% 
    mutate_at(vars(sign), as.factor)


sig.clusters
```

We see one significant cluster starting from 500 ms to 1000 ms. During this time period the spanish cohort effect is significant.

```{r}
# Filter data for Spanish-Spanish condition
gaze_sub_L2_SPSP <- gaze_sub_L2 %>%
    filter(condition == "TCUU-SPSP")

## Plot the Spanish-Spanish condition
tcru <- plot_IA_proportions(
    gaze_sub_L2_SPSP, 
    ia_column = "condition1",
    time_column = "time_bin",
    proportion_column = "Fix",
    condition_column = "condition", 
    ia_mapping = list(target = "Target", cohort = "Cohort", unrelated = "Unrelated")
) +
    facet_wrap(
        ~ condition,             # Facet by condition
        labeller = labeller(
            condition = c(
                "TCUU-SPSP" = "Spanish-Spanish Condition"
            )
        )
    ) +
    labs(tag = "a") +         # Optional tag for labeling
    theme(
        strip.text = element_text(size = 14),  # Adjust facet label text size
        panel.spacing = unit(1, "lines")       # Add space between panels
    ) +
    # Add the error bar from 500 ms to 1000 ms
    geom_errorbarh(aes(xmin = 500, xmax = 1000, y = 0), 
                   height = 0.05, size = 3, alpha = 0.4, colour = "red")

# Print the plot
tcru


```

```{r}

gaze_sub_L2_cp <- gaze_sub_L2 %>%
  filter(condition=="TCUU-SPENG", condition1=="cohort" | condition1=="unrelated") %>%
  mutate(condition1_code=ifelse(condition1=="cohort", 0.5, -0.5))

```

```{r}

library(permutes)
library(permuco)
library(foreach) # for par 

cpa.glmer = permutes::clusterperm.glmer(Looks~ condition1_code + (1|subject) + (1|trial), data=gaze_sub_L2_cp, series.var=~time_bin, nperm = 1000, parallel=TRUE)


```

```{r}
sig.clusters = cpa.glmer %>% filter(Factor=='condition1_code', !is.na(cluster_mass), p.cluster_mass<.05) %>% 
    mutate_at(vars(time_bin), as.numeric) %>% group_by(cluster) %>% 
    summarise(cluster_mass=min(cluster_mass), p.cluster_mass=min(p.cluster_mass), bin_start=min(time_bin), bin_end=max(time_bin), t=mean(t)) %>% 
    mutate(sign=ifelse(t<0,-1,1), time_start=(bin_start-2)*100, time_end=(bin_end-2)*100) %>% 
    mutate_at(vars(sign), as.factor)


sig.clusters
```

```{r}
# Filter data for Spanish-Spanish condition
gaze_sub_L2_SPENG <- gaze_sub_L2 %>%
    filter(condition == "TCUU-SPENG")

## Plot the Spanish-Spanish condition
tcru <- plot_IA_proportions(
    gaze_sub_L2_SPENG, 
    ia_column = "condition1",
    time_column = "time_bin",
    proportion_column = "Fix",
    condition_column = "condition", 
    ia_mapping = list(target = "Target", cohort = "Cohort", unrelated = "Unrelated")
) +
    facet_wrap(
        ~ condition,             # Facet by condition
        labeller = labeller(
            condition = c(
                "TCUU-SPSP" = "Spanish-English Condition"
            )
        )
    ) +
    labs(tag = "a") +         # Optional tag for labeling
    theme(
        strip.text = element_text(size = 14),  # Adjust facet label text size
        panel.spacing = unit(1, "lines")       # Add space between panels
    ) +
    # Add the error bar from 500 ms to 1000 ms
    geom_errorbarh(aes(xmin = 200, xmax = 400, y = 0), 
                   height = 0.05, size = 3, alpha = 0.4, colour = "red")

# Print the plot

tcru 

```

# Discussion

One main issue with webcam eye-tracking, at lest with webgazeR is the

## Questionnaire Responses

After the experiment we asked all participants to answer several questions as it pertained to their experimental set-up and their overall experience with the eye-tracking experiment. All questions are in @tbl-question.

## Poor calibrators vs. Good calibrators

Looking at folks that did successfully calibrate, it appears that of visipn problems was always answered yes. In additon how far from computer

```{r}
#| label: tbl-eyequest
#| echo: false

eye_quest <- read_csv(here::here("data", "monolinguals", "data_exp_189729-v5_questionnaire-7ac3.csv"))

eye_quest2 <- read_csv(here::here("data", "monolinguals", "data_exp_189729-v6_questionnaire-7ac3.csv"))
  
  
agg_eye_data_quest <- rbind(eye_quest, eye_quest2) # bind the two datasets together

 eye_quests  <- agg_eye_data_quest %>% select(`Participant Private ID`, Question, Response, Key) %>% filter(Key=="value")
```

```{r}
#| label: tbl-question
#| tbl-cap: Eye-tracking Questionaire Items
#| echo: false
#| 
eye_quests %>% select(Question,Key) %>% filter(Key=="value") %>% select(-Key) %>%distinct() %>% tt()

```

```{r}
qq_bad <- eye_quests %>%
    filter(!`Participant Private ID` %in% trial_data_rt$subject)

qq_good <- eye_quests %>%
    filter(`Participant Private ID` %in% trial_data_rt$subject)

```

```{r}
#| label: tbl-goodbad
#| echo: false
#| tbl-cap: Responses to eye-tracking questions for participants who successfully calibrated vs. participants who had trouble calibratring
#| echo: false
# Calculate percentages for good participants
# Define the cleaning function
clean_responses <- function(response) {
    response <- str_replace_all(response, "[[:punct:]]", "") %>%  # Remove punctuation
        str_to_lower()                                              # Convert to lowercase
    case_when(
        response == "yes" ~ "Yes",                  # Standardize "yes"
        response == "no" ~ "No",                    # Standardize "no"
        response %in% c("na", "n/a", "non") ~ "NA", # Standardize NA variants
        response == "zero" ~ "0",                   # Convert "zero" to "0"
        TRUE ~ response                             # Leave other responses unchanged
    )
}

# Calculate percentages of responses per question for good participants
percentage_responses_good <- qq_good %>%
    group_by(Question, Response) %>%
    summarise(count_good = n(), .groups = "drop_last") %>%  # Count occurrences for each Response
    mutate(
        total_responses_good = sum(count_good),         # Total responses per Question
        percentage_good = (count_good / total_responses_good) * 100,  # Calculate percentage
        Response = clean_responses(Response)           # Clean responses
    ) %>%
    ungroup()

# Calculate percentages of responses per question for bad participants
percentage_responses_bad <- qq_bad %>%
    group_by(Question, Response) %>%
    summarise(count_bad = n(), .groups = "drop_last") %>%  # Count occurrences for each Response
    mutate(
        total_responses_bad = sum(count_bad),          # Total responses per Question
        percentage_bad = (count_bad / total_responses_bad) * 100,  # Calculate percentage
        Response = clean_responses(Response)          # Clean responses
    ) %>%
    ungroup()

# Combine the data
percentage_responses_combined <- full_join(
    percentage_responses_good %>% select(Question, Response, percentage_good),
    percentage_responses_bad %>% select(Question, Response, percentage_bad),
    by = c("Question", "Response")
)

# Replace NA with 0 (if needed)
percentage_responses_combined <- percentage_responses_combined %>%
    mutate(
        percentage_good = replace_na(percentage_good, 0),
        percentage_bad = replace_na(percentage_bad, 0)
    )

# Remove questions 11 and 12
percentage_responses_filtered <- percentage_responses_combined %>%
    filter(!Question %in% c("11. What additional information would you add to help make things easier to understand?",
                            "12. Are you wearing a mask?"))

# Arrange and finalize the table
final_table <- percentage_responses_filtered %>%
    arrange(Question, Response)

# View the final cleaned table
print(final_table)

# Optional: Export the final table for publication
write.csv(final_table, "final_cleaned_data_no_questions_11_12.csv", row.names = FALSE)
```

```{r}
#| echo: false

eye_quest_good <- read_csv(here::here("data", "L2", "data_exp_196386-v5_questionnaire-7ac3.csv"))

eye_quest_bad <- read_csv(here::here("data", "L2", "data_exp_196386-v5_questionnaire-ng98.csv"))
  
  
 eye_quests_bad <- eye_quest_bad %>% select(`Participant Private ID`, Question, Response, Key) %>% filter(Key=="value")

```

```{r}

clean_responses <- function(response) {
    response <- str_replace_all(response, "[[:punct:]]", "") %>%  # Remove punctuation
        str_to_lower()                                              # Convert to lowercase
    case_when(
        response == "yes" ~ "Yes",                  # Standardize "yes"
        response == "no" ~ "No",                    # Standardize "no"
        response %in% c("na", "n/a", "non") ~ "NA", # Standardize NA variants
        response == "zero" ~ "0",                   # Convert "zero" to "0"
        TRUE ~ response                             # Leave other responses unchanged
    )
}

# Calculate percentages of responses per question for good participants
percentage_responses_good <- eye_quests_bad %>%
    group_by(Question, Response) %>%
    summarise(count_good = n(), .groups = "drop_last") %>%  # Count occurrences for each Response
    mutate(
        total_responses_good = sum(count_good),         # Total responses per Question
        percentage_good = (count_good / total_responses_good) * 100,  # Calculate percentage
        Response = clean_responses(Response)           # Clean responses
    ) %>%
    ungroup()



```
